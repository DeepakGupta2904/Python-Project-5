{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44572e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CodSoft Internship Programme for Data Science (1 October 2023 to 31 October 2023) #####\n",
    "##### Name:- Deepak Gupta #####\n",
    "##### Task-3 IRIS Flower Classification ##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ebb3b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Import Some Important Libraries:-\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "354817ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Import the dataset:-\n",
    "iris = pd.read_csv(\"IRIS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef04c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Let's see what's in the iris data:-\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed79ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a507fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Analysing the data visually:-\n",
    "##### At the outset , let us look at a simple scatter plot , to get a visual feel of the data. (We are going to view a host of \n",
    "##### them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### We'll use this to make a scatterplot of the Iris features.\n",
    "iris.plot(kind=\"scatter\", x=\"SepalLengthCm\", y=\"SepalWidthCm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4748b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### A seaborn jointplot shows bivariate scatterplots and univariate histograms in the same figure\n",
    "sns.jointplot(x=\"SepalLengthCm\", y=\"SepalWidthCm\", data=iris, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee67f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### We'll use seaborn's FacetGrid to color the scatterplot by species\n",
    "sns.FacetGrid(iris, hue=\"Species\", size=5) \\\n",
    "   .map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\") \\\n",
    "   .add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Secondly , let us see at the box plot of the dataset, which shows us the visual representation of how our data is \n",
    "##### scattered over the the plane. Box plot is a percentile-based graph, which divides the data into four quartiles of 25% \n",
    "##### each. This method is used in statistical analysis to understand various measures such as mean, median and deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d47afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### We can look at an individual feature in Seaborn through a boxplot\n",
    "sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e22f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One way we can extend this plot is adding a layer of individual points on top of it through Seaborn's striplot\n",
    "\n",
    "## We'll use jitter=True so that all the points don't fall in single vertical lines above the species\n",
    "## Saving the resulting axes as ax each time causes the resulting plot to be shown on top of the previous axes\n",
    "\n",
    "ax = sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=iris)\n",
    "ax = sns.stripplot(x=\"Species\", y=\"PetalLengthCm\", data=iris, jitter=True, edgecolor=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a special plot called violin plot\n",
    "\n",
    "## A violin plot combines the benefits of the previous two plots and simplifies them Denser regions of the data are fatter, and\n",
    "## sparser thiner in a violin plot\n",
    "sns.violinplot(x=\"Species\", y=\"PetalLengthCm\", data=iris, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f385c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next is a visual based on probability density , called kernel density plots. (KD Plots)\n",
    "\n",
    "## A final seaborn plot useful for looking at univariate relations is the kdeplot,which creates and visualizes a kernel density\n",
    "## estimate of the underlying feature\n",
    "\n",
    "sns.FacetGrid(iris, hue=\"Species\", size=6) \\\n",
    "   .map(sns.kdeplot, \"PetalLengthCm\") \\\n",
    "   .add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Another useful seaborn plot is a hybrid plot called pairplot, which shows the bivariate relation between each pair of \n",
    "##### features. Lets see the same:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From the pairplot, we'll see that the Iris-setosa species is separataed from the other two across all feature combinations\n",
    "\n",
    "sns.pairplot(iris.drop(\"Id\", axis=1), hue=\"Species\", size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e36553",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Box plot grid\n",
    "iris.drop(\"Id\", axis=1).boxplot(by=\"Species\", figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a6cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### And now , let's see some special visuals !! One cool more sophisticated technique pandas has available is called Andrews \n",
    "##### Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Andrews Curves involve using attributes of samples as coefficients for Fourier series and then plotting these\n",
    "\n",
    "from pandas.plotting import andrews_curves\n",
    "andrews_curves(iris.drop(\"Id\", axis=1), \"Species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another multivariate visualization technique pandas has is parallel_coordinates Parallel coordinates plots each feature on a \n",
    "## separate column & then draws lines connecting the features for each data sample\n",
    "\n",
    "from pandas.plotting import parallel_coordinates\n",
    "parallel_coordinates(iris.drop(\"Id\", axis=1), \"Species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6346a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### A final multivariate visualization technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Which puts each feature as a point on a 2D plane, and then simulates having each sample attached to those points through a \n",
    "## spring weighted by the relative value for that feature\n",
    "\n",
    "from pandas.plotting import radviz\n",
    "radviz(iris.drop(\"Id\", axis=1), \"Species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b66f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INTO THE REALM OF MACHINE LEARNING:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df9e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seperating the data into dependent and independent variables\n",
    "X = iris.iloc[:, :-1].values\n",
    "y = iris.iloc[:, -1].values\n",
    "\n",
    "## Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b52264",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Training the model:-\n",
    "##### Using some of the commonly used algorithms, we will be training our model to check how accurate every algorithm is.We will\n",
    "##### be implementing these algorithms to compare:-\n",
    "##### 1. Logistic Regression\n",
    "##### 2. K â€“ Nearest Neighbour (KNN)\n",
    "##### 3. Support Vector Machine (SVM)\n",
    "##### 4. Decision Trees\n",
    "##### 5. Naive Bayes classifier\n",
    "\n",
    "##### Let us start building our model and predicting accuracy of every algorithm used. We can also check which gives the best \n",
    "##### result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start with the first algorithm Logistic Regression:-\n",
    "\n",
    "##### LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "##### Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "##### Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf23d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Now , let us see the scores with K-Nearest Neighbors technique.\n",
    "\n",
    "##### K-Nearest Neighbours\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=8)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "##### Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "##### Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fab00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Thirdly , with SVM (Support Vector Machines).\n",
    "\n",
    "##### Support Vector Machine's \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "##### Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "##### Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7606b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Next , is my favorite , decision trees !\n",
    "\n",
    "##### Decision Tree's\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "##### Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "##### Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf7016",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### And lastly , the Naive Bayes classifier. (Variants included)\n",
    "\n",
    "##### Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "##### Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "\n",
    "##### Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "##### Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "##### Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a670dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Bernoulli Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifier = BernoulliNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "##### Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "##### Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Complement Naive Bayes\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "classifier = ComplementNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "##### Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "##### Accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    MultinomialNB(),\n",
    "    BernoulliNB(),\n",
    "    ComplementNB(),               \n",
    "                  ]\n",
    " \n",
    "##### Logging for Visual Comparison\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    " \n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, acc*100, 11]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    \n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76244517",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n",
    "\n",
    "plt.xlabel('Accuracy %')\n",
    "plt.title('Classifier Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c57b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
